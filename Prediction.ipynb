{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d35130",
   "metadata": {},
   "source": [
    "# Partie sur la prediction des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc97542",
   "metadata": {},
   "source": [
    "## Classification des elections du second tour 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd753c08",
   "metadata": {},
   "source": [
    "### Application de SVM, en faisant du copier coller du tp8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a296d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55e820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Departement nbPersonne     Macron      Lepen niveauEtude  \\\n",
      "0                       Ain     671937  50.982366  49.017659        41.4   \n",
      "1                     Aisne     522791  38.089161  61.910914        29.7   \n",
      "2                    Allier     332443  48.201987  51.798013        33.4   \n",
      "3   Alpes-de-Haute-Provence     166654  47.890556  52.109697        37.2   \n",
      "4              Hautes-Alpes     139942  53.414383  46.585802        41.7   \n",
      "..                      ...        ...        ...        ...         ...   \n",
      "89    Territoire de Belfort     136891  45.499208  54.500792        43.3   \n",
      "90                  Essonne    1316053  58.871340  41.128711        47.5   \n",
      "91           Hauts-de-Seine    1642002  80.148611  19.851389        71.1   \n",
      "92             Val-de-Marne    1426748  71.616596  28.383404        56.1   \n",
      "93               Val-d'Oise    1274374  58.109402  41.890707        45.2   \n",
      "\n",
      "    tauxChomage Proximité   Immigré  \n",
      "0           5.5      1.66   79312.0  \n",
      "1          10.5      0.83   25515.0  \n",
      "2           7.7      2.39   17601.0  \n",
      "3           8.2      0.96   13762.0  \n",
      "4           6.9      5.43    8019.0  \n",
      "..          ...       ...       ...  \n",
      "89          8.3      6.09   13875.0  \n",
      "90          6.4      3.34  224344.0  \n",
      "91          5.9      4.19  307362.0  \n",
      "92          7.1      2.92  311186.0  \n",
      "93          8.0      1.53  253856.0  \n",
      "\n",
      "[94 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#creation du dataset\n",
    "df_pourcent_chomage = pd.read_excel('AutreData/tauxChomage.xlsx').dropna()\n",
    "df_pourcent_chomage = df_pourcent_chomage.iloc[:, [1, 2]]\n",
    "df_pourcent_chomage.columns = ['Departement', 'tauxChomage']\n",
    "\n",
    "df_niveauEtude = pd.read_excel('AutreData/NiveauEtudeJeune2017.xlsx', sheet_name='Figure 1a').dropna()\n",
    "df_niveauEtude = df_niveauEtude.iloc[:, [1, 2]]\n",
    "df_niveauEtude.columns = ['Departement', 'niveauEtude']\n",
    "\n",
    "df_densitePop = pd.read_excel('AutreData/NbPopDepartement2023.xls', sheet_name='2023').dropna()\n",
    "df_densitePop = df_densitePop.iloc[:, [1, 7]]\n",
    "df_densitePop.columns = ['Departement', 'nbPersonne']\n",
    "\n",
    "df_magasinBIO = pd.read_excel('AutreData/MagasinBio2016.xls', sheet_name='Figure 2').dropna()\n",
    "df_magasinBIO.columns = ['Departement', 'Proximité']\n",
    "\n",
    "df_PopImmigre = pd.read_excel('AutreData/PopImmigré.xlsx').dropna()\n",
    "df_PopImmigre = df_PopImmigre.iloc[:, [1, 2]]\n",
    "df_PopImmigre.columns = ['Departement', 'Immigré']\n",
    "\n",
    "# Charger les données des résultats présidentiels\n",
    "df_resultatPresidentielle = pd.read_excel('resultats-par-niveau-subcom-t2-france-entiere.xlsx').dropna()\n",
    "df_resultatPresidentielle = df_resultatPresidentielle.iloc[:, [1, 25, 32]]\n",
    "df_resultatPresidentielle.columns = [ 'Departement', 'Macron', 'Lepen']\n",
    "df_resultatPresidentielle = df_resultatPresidentielle.groupby('Departement')[['Macron', 'Lepen']].mean().reset_index()\n",
    "\n",
    "df_concat = pd.merge(df_densitePop, df_resultatPresidentielle, on='Departement')\n",
    "df_concat = pd.merge(df_concat, df_niveauEtude, on='Departement')\n",
    "df_concat = pd.merge(df_concat, df_pourcent_chomage, on='Departement')\n",
    "df_concat = pd.merge(df_concat, df_magasinBIO, on='Departement')\n",
    "df_concat = pd.merge(df_concat, df_PopImmigre, on='Departement')\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0844174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des features\n",
    "features = df_concat.drop(['Departement','Macron', 'Lepen'], axis=1)\n",
    "y = [0 if x <= 50 else 1 for x in df_concat['Macron']]\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "#division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942aaf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du SVM avec un noyau linéaire :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         9\n",
      "           1       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.88      0.83      0.83        19\n",
      "weighted avg       0.88      0.84      0.84        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur les données de test et évaluer les performances\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "print(\"Performance du SVM avec un noyau linéaire :\")\n",
    "print(classification_report(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65989fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du SVM avec un noyau linéaire :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72         9\n",
      "           1       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.63        19\n",
      "   macro avg       0.78      0.65      0.59        19\n",
      "weighted avg       0.79      0.63      0.58        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_linear = SVC(kernel='poly')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur les données de test et évaluer les performances\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "print(\"Performance du SVM avec un noyau linéaire :\")\n",
    "print(classification_report(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0077c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour SVM: {'C': 10, 'gamma': 0.1}\n",
      "Performance du SVM avec les meilleurs paramètres :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75         9\n",
      "           1       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.80      0.78      0.78        19\n",
      "weighted avg       0.80      0.79      0.79        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test)\n",
    "print(\"Meilleurs paramètres pour SVM:\", grid_search.best_params_)\n",
    "print(\"Performance du SVM avec les meilleurs paramètres :\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e57ebe",
   "metadata": {},
   "source": [
    "### Application de Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ccf7aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75         9\n",
      "           1       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.80      0.78      0.78        19\n",
      "weighted avg       0.80      0.79      0.79        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# régression logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logReg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a656cdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.85      0.84      0.84        19\n",
      "weighted avg       0.85      0.84      0.84        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier using Ridge regression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridgeClf = RidgeClassifier()\n",
    "ridgeClf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ridgeClf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1d977f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.62         9\n",
      "           1       0.67      1.00      0.80        10\n",
      "\n",
      "    accuracy                           0.74        19\n",
      "   macro avg       0.83      0.72      0.71        19\n",
      "weighted avg       0.82      0.74      0.71        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "\n",
    "sgdClf = SGDClassifier()\n",
    "sgdClf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgdClf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294b1db",
   "metadata": {},
   "source": [
    "### Application de Foret aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3b55b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.89        19\n",
      "   macro avg       0.92      0.89      0.89        19\n",
      "weighted avg       0.91      0.89      0.89        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Entraînement d'une forêt aléatoire\n",
    "forest_model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred_forest = forest_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bc5a8",
   "metadata": {},
   "source": [
    "### Reseau de neuronne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fa93529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8156 - accuracy: 0.4000\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 966us/step - loss: 0.8071 - accuracy: 0.4400\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7989 - accuracy: 0.4667\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7904 - accuracy: 0.4533\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7820 - accuracy: 0.4667\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7754 - accuracy: 0.4400\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7672 - accuracy: 0.4533\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7606 - accuracy: 0.4533\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7528 - accuracy: 0.4533\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7466 - accuracy: 0.4667\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7399 - accuracy: 0.4933\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7333 - accuracy: 0.4933\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 952us/step - loss: 0.7276 - accuracy: 0.4800\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7215 - accuracy: 0.4800\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 939us/step - loss: 0.7162 - accuracy: 0.4800\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7113 - accuracy: 0.5200\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 996us/step - loss: 0.7067 - accuracy: 0.5067\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.5067\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 934us/step - loss: 0.6977 - accuracy: 0.5067\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5200\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 963us/step - loss: 0.6893 - accuracy: 0.5067\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5067\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6815 - accuracy: 0.5067\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.5067\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6741 - accuracy: 0.5200\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.5333\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6673 - accuracy: 0.5467\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6639 - accuracy: 0.5467\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 946us/step - loss: 0.6610 - accuracy: 0.5600\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.5600\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6551 - accuracy: 0.5600\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 924us/step - loss: 0.6524 - accuracy: 0.5867\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 895us/step - loss: 0.6495 - accuracy: 0.5867\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 928us/step - loss: 0.6469 - accuracy: 0.6133\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6133\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 941us/step - loss: 0.6418 - accuracy: 0.6133\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 966us/step - loss: 0.6392 - accuracy: 0.6133\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 983us/step - loss: 0.6369 - accuracy: 0.6133\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6345 - accuracy: 0.6133\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6320 - accuracy: 0.6133\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 979us/step - loss: 0.6301 - accuracy: 0.6133\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 959us/step - loss: 0.6276 - accuracy: 0.6267\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.6258 - accuracy: 0.6267\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6235 - accuracy: 0.6267\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 967us/step - loss: 0.6217 - accuracy: 0.6267\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6201 - accuracy: 0.6533\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 962us/step - loss: 0.6182 - accuracy: 0.6533\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6164 - accuracy: 0.6533\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6148 - accuracy: 0.6533\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6131 - accuracy: 0.6533\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x30abd9080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x30abd9080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.78      0.67         9\n",
      "           1       0.71      0.50      0.59        10\n",
      "\n",
      "    accuracy                           0.63        19\n",
      "   macro avg       0.65      0.64      0.63        19\n",
      "weighted avg       0.65      0.63      0.63        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses\n",
    "import numpy as np\n",
    "\n",
    "X_trainNN = np.array(X_train)\n",
    "y_trainNN = np.array(y_train)\n",
    "X_testNN = np.array(X_test)\n",
    "y_testNN = np.array(y_test)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(features.shape[1], activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.binary_crossentropy,\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])     \n",
    "\n",
    "history = model.fit(X_trainNN, y_trainNN,\n",
    "                    epochs=50,\n",
    "                   )\n",
    "\n",
    "y_pred_NN = model.predict(X_testNN)\n",
    "y_pred_binary = (y_pred_NN > 0.5).astype(int)\n",
    "print(classification_report(y_testNN, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369eae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
